# RagArchivarius

## Обзор

Этот проект представляет собой RAG-систему (Retrieval-Augmented Generation), предназначенную для ответов на вопросы по истории кафедры университета (или любой другой предметной области) на основе базы знаний из исторических документов. Он оснащен интерфейсом Telegram-бота для взаимодействия с пользователем и использует как локальные, так и облачные большие языковые модели (LLM) для различных этапов процесса.

Система использует локальную LLM (запущенную через LM Studio) для первоначального анализа запросов и мощную облачную LLM (GigaChat) для синтеза окончательных ответов на основе полученного контекста, дополняя их ссылками на источники.

## Архитектура

1.  **Ввод пользователя:** Получение запроса через Telegram-бот или консоль.
2.  **Анализ запроса (Локальная LLM):**
    *   Запрос пользователя и недавняя история диалога отправляются в локальную LLM (через API LM Studio).
    *   Системный промпт направляет LLM на классификацию запроса (`question`/`banter`) и извлечение `person_tag` (имени личности) с использованием динамически загружаемого списка известных имен из Qdrant.
    *   LLM возвращает JSON-объект с полями `query_type`, `person_tag` и `refined_query`.
3.  **(Если 'question') Векторный поиск (Qdrant):**
    *   `refined_query` преобразуется в векторное представление (эмбеддинг).
    *   В Qdrant выполняется поиск релевантных фрагментов документов с использованием вектора запроса.
    *   Поиск фильтруется по `person_tag` (если он был извлечен).
4.  **(Если 'question') Генерация ответа (GigaChat):**
    *   Извлеченные фрагменты документов (контекст), оригинальный запрос пользователя, история диалога и системный промпт (инструктирующий GigaChat действовать как архивариус и ссылаться на источники) отправляются в GigaChat API.
    *   GigaChat генерирует окончательный ответ.
5.  **(Если 'banter') Прямой ответ:** Возвращается простой, заранее определенный дружелюбный ответ.
6.  **Вывод:** Сгенерированный ответ отправляется пользователю через Telegram или в консоль.
7.  **Обновление истории:** История диалога обновляется для следующего шага.

---

## Полное руководство по установке и запуску

Это пошаговое руководство поможет вам развернуть проект на вашем локальном компьютере.

### Шаг 1: Подготовка окружения

Вам понадобятся:
*   **Docker Desktop:** Для запуска базы данных Qdrant и самого бота. Убедитесь, что он установлен и запущен.
*   **Git:** Для клонирования репозитория.
*   **Python:** Версия 3.10 или выше.

### Шаг 2: Клонирование репозитория
Откройте терминал или командную строку и выполните:
```bash
git clone https://github.com/maratfazylov/RagArchivarius.git
cd RagArchivarius
```

### Шаг 3: Настройка переменных окружения

Создайте файл `.env` в корневой папке проекта. Он будет содержать все ваши секретные ключи.

1.  Скопируйте пример:
    *   **Windows (в cmd):** `copy .env.example .env`
    *   **Windows (в PowerShell):** `cp .env.example .env`
    *   **Linux/macOS:** `cp .env.example .env`

2.  Откройте файл `.env` и вставьте ваши реальные значения:
    ```dotenv
    # Токен, полученный от @BotFather в Telegram
    TELEGRAM_BOT_TOKEN="ВАШ_ТЕЛЕГРАМ_ТОКЕН"

    # Ваши учетные данные для GigaChat API
    GIGACHAT_CLIENT_ID="ВАШ_GIGACHAT_CLIENT_ID"
    GIGACHAT_CLIENT_SECRET="ВАШ_GIGACHAT_СЕКРЕТ"

    # IP-адрес для LM Studio (обновляется на шаге 7)
    LOCAL_LLM_SERVER_IP="127.0.0.1"
    ```

### Шаг 4: Запуск базы данных Qdrant

В корневой папке проекта выполните команду, чтобы запустить сервис Qdrant в Docker:
```bash
docker-compose up -d qdrant
```
Эта команда скачает образ Qdrant (если его нет) и запустит его в фоновом режиме. Вы можете проверить, что он работает, открыв в браузере `http://localhost:6333/dashboard`.

### Шаг 5: Добавление документов в базу знаний

Это самый важный шаг для обучения вашего бота.

1.  **Подготовьте документы:**
    *   Найдите папку `raw_text` в проекте.
    *   Поместите в нее ваши текстовые файлы в формате `.txt`.
    *   **Ключевое правило:** Называйте файлы по имени личности, которой они посвящены, в формате `Фамилия Имя Отчество.txt` (например, `Лобачевский Николай Иванович.txt`). Это имя будет использоваться как главный тег (`person_tag`) для фильтрации при поиске.

2.  **Запустите скрипт загрузки:**
    *   Убедитесь, что у вас установлены необходимые Python-библиотеки (если вы запускаете скрипт вне Docker):
        ```bash
        pip install -r requirements.txt
        ```
    *   Выполните скрипт `load_data_to_qdrant.py`:
        ```bash
        python load_data_to_qdrant.py
        ```
    Скрипт прочитает все файлы из `raw_text`, разобьет их на фрагменты, создаст векторные представления и загрузит их в Qdrant. Вы увидите прогресс в консоли.

### Шаг 6: Установка и настройка локальной LLM (LM Studio)

Локальная модель нужна для анализа запросов.

1.  **Скачайте и установите** [LM Studio](https://lmstudio.ai/) для вашей операционной системы.
2.  **Найдите и скачайте модель:**
    *   Откройте LM Studio.
    *   На главной странице в поиске введите `Qwen 1.5` и нажмите Enter.
    *   В результатах найдите модель от `Qwen` (например, `Qwen/Qwen1.5-7B-Chat-GGUF`).
    *   Справа выберите и скачайте один из квантованных файлов (например, `qwen1_5-7b-chat-q5_K_M.gguf` — хороший баланс качества и производительности).
3.  **Запустите локальный сервер:**
    *   Перейдите на вкладку "Local Server" (иконка `<->` слева).
    *   Вверху в выпадающем меню "Select a model to load" выберите скачанную модель `Qwen`.
    *   Нажмите кнопку **Start Server**.
    *   После запуска сервера в консоли вверху найдите строки, указывающие IP-адрес и порт, которые слушает сервер (например, `http://192.168.1.5:8000`).

### Шаг 7: Запуск бота

1.  **Определите IP-адрес вашего компьютера:**
    Бот, работая в Docker, должен знать, как подключиться к LM Studio, которая запущена на вашем ПК.
    *   **Windows:** Откройте `cmd` и введите `ipconfig`. Найдите ваш локальный IP-адрес (обычно в секции "Wireless LAN adapter Wi-Fi" или "Ethernet adapter").
    *   **macOS/Linux:** Откройте терминал и введите `ifconfig` или `ip addr`.
2.  **Обновите `.env` файл:**
    *   Откройте файл `.env`.
    *   Измените значение `LOCAL_LLM_SERVER_IP` на IP-адрес, который вы нашли на предыдущем шаге. Порт должен совпадать с тем, что в LM Studio.

3.  **Запустите все вместе:**
    Теперь, когда Qdrant запущен с данными, а сервер LM Studio работает, выполните команду для сборки и запуска вашего бота:
    ```bash
    docker-compose up --build
    ```
    Ключ `--build` пересобирает образ вашего бота, включая все последние изменения в коде.

### Шаг 8: Проверка и остановка

*   **Проверка логов:** Чтобы убедиться, что все работает, и отслеживать ошибки, откройте новый терминал и выполните:
    ```bash
    docker-compose logs -f rag-bot
    ```
*   **Остановка:** Чтобы остановить все сервисы (бота и Qdrant), нажмите `Ctrl+C` в терминале, где запущен `docker-compose up`, или выполните в другом терминале:
    ```bash
    docker-compose down
    ```

## Потенциальные улучшения

*   **Управление контекстным окном:** Реализовать динамическое усечение или суммирование истории диалога для LLM.
*   **Надежная обработка `person_tag`:** Улучшить промпты или добавить логику постобработки для более надежного извлечения и нормализации имен.
*   **Файл конфигурации:** Переместить константы из `rag_agent.py` в файл конфигурации (например, `config.yaml`) или `.env`.
*   **Асинхронные операции:** Сделать бота полностью асинхронным, чтобы избежать блокировок при длительных вызовах LLM/Qdrant.
*   **Конвейер предварительной обработки документов:** Формализовать процесс загрузки, разбиения на части и тегирования документов.
*   **Фреймворк для оценки:** Внедрить метрики и тесты для оценки качества извлечения и генерации.
